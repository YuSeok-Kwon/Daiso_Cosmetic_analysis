"""
ABSA íŒ¨í„´ ê¸°ë°˜ ì¬ë¶„ì„
1ë‹¨ê³„: ìƒ˜í”Œ 100ê°œ GPT-5 ë¶„ì„ â†’ aspectë³„ sentiment íŒ¨í„´ ì¶”ì¶œ
2ë‹¨ê³„: íŒ¨í„´ì„ ì „ì²´ ë°ì´í„°ì— ì ìš©
"""

import sys
import os
import json
import re
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict
from openai import OpenAI

project_root = Path(__file__).parent.parent
aspect_dir = project_root / "data" / "raw" / "by_aspect"
output_dir = project_root / "data" / "processed"
output_dir.mkdir(parents=True, exist_ok=True)

ASPECTS = [
    "ë°°ì†¡/í¬ì¥", "í’ˆì§ˆ/ë¶ˆëŸ‰", "ê°€ê²©/ê°€ì„±ë¹„", "ì‚¬ìš©ê°/ì„±ëŠ¥",
    "ë””ìì¸", "ì¬ì§ˆ/ëƒ„ìƒˆ", "CS/ì‘ëŒ€", "ì¬êµ¬ë§¤", "ìƒ‰ìƒ/ë°œìƒ‰", "ìš©ëŸ‰/íœ´ëŒ€"
]

FILE_TO_ASPECT = {
    "ë°°ì†¡_í¬ì¥": "ë°°ì†¡/í¬ì¥",
    "í’ˆì§ˆ": "í’ˆì§ˆ/ë¶ˆëŸ‰",
    "ê°€ê²©_ê°€ì„±ë¹„": "ê°€ê²©/ê°€ì„±ë¹„",
    "ì‚¬ìš©ê°_ì„±ëŠ¥": "ì‚¬ìš©ê°/ì„±ëŠ¥",
    "ë””ìì¸": "ë””ìì¸",
    "ì¬ì§ˆ_ëƒ„ìƒˆ": "ì¬ì§ˆ/ëƒ„ìƒˆ",
    "CS_ì‘ëŒ€": "CS/ì‘ëŒ€",
    "ì¬êµ¬ë§¤": "ì¬êµ¬ë§¤",
    "ìƒ‰ìƒ_ë°œìƒ‰": "ìƒ‰ìƒ/ë°œìƒ‰",
    "ìš©ëŸ‰_íœ´ëŒ€": "ìš©ëŸ‰/íœ´ëŒ€"
}


def extract_json(text: str) -> dict:
    """ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ"""
    if not text:
        return None
    # JSON ë¸”ë¡ ì°¾ê¸°
    json_match = re.search(r'\{[\s\S]*\}', text)
    if json_match:
        try:
            return json.loads(json_match.group())
        except:
            pass
    try:
        return json.loads(text)
    except:
        return None


def analyze_with_gpt5(client: OpenAI, text: str, rating: int) -> dict:
    """GPT-5ë¡œ ë¦¬ë·°ì˜ aspectë³„ sentiment ë¶„ì„"""
    prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ì‡¼í•‘ëª° ë¦¬ë·°ì˜ ABSA(Aspect-Based Sentiment Analysis) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ë¦¬ë·°]
"{text}"

[ë³„ì ]
{rating}ì  (5ì  ë§Œì )

[ê°€ëŠ¥í•œ Aspect]
ë°°ì†¡/í¬ì¥, í’ˆì§ˆ/ë¶ˆëŸ‰, ê°€ê²©/ê°€ì„±ë¹„, ì‚¬ìš©ê°/ì„±ëŠ¥, ë””ìì¸, ì¬ì§ˆ/ëƒ„ìƒˆ, CS/ì‘ëŒ€, ì¬êµ¬ë§¤, ìƒ‰ìƒ/ë°œìƒ‰, ìš©ëŸ‰/íœ´ëŒ€

[ì‘ì—…]
ì´ ë¦¬ë·°ì—ì„œ ì–¸ê¸‰ëœ aspectì™€ ê°ê°ì˜ sentimentë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
- ì‹¤ì œë¡œ ì–¸ê¸‰ëœ aspectë§Œ ì¶”ì¶œ
- ê° aspectë³„ë¡œ ë…ë¦½ì ì¸ sentiment íŒë‹¨
- sentiment: positive/neutral/negative
- score: -1.0 ~ 1.0

[ì¶œë ¥ - JSON]
{{
  "aspects": [
    {{
      "aspect": "aspectëª…",
      "sentiment": "positive/neutral/negative",
      "score": 0.8,
      "keywords": ["íŒë‹¨ ê·¼ê±° í‚¤ì›Œë“œ"],
      "opinion": "í•´ë‹¹ aspectì— ëŒ€í•œ ì˜ê²¬ í‘œí˜„"
    }}
  ]
}}

ë°˜ë“œì‹œ ë¦¬ë·°ì—ì„œ ì‹¤ì œë¡œ ì–¸ê¸‰ëœ aspectë§Œ í¬í•¨í•˜ì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="gpt-5",
            messages=[{"role": "user", "content": prompt}],
            max_completion_tokens=800,
            response_format={"type": "json_object"}
        )
        result = extract_json(response.choices[0].message.content)
        if result:
            tokens = response.usage.prompt_tokens + response.usage.completion_tokens
            result["cost"] = tokens * 0.01 / 1000
        return result
    except Exception as e:
        print(f"Error: {e}")
        return None


def extract_patterns_from_samples(client: OpenAI, all_dfs: dict, sample_ratio: float = 0.1, max_samples: int = 300):
    """ìƒ˜í”Œì—ì„œ aspect-sentiment íŒ¨í„´ ì¶”ì¶œ (ê° íŒŒì¼ì˜ 10%, ìµœëŒ€ 300ê°œ)"""
    # {aspect: {sentiment: [keywords]}}
    patterns = defaultdict(lambda: defaultdict(list))
    all_samples = []
    total_cost = 0

    for file_name, df in all_dfs.items():
        sample_size = max(10, min(int(len(df) * sample_ratio), max_samples))  # ìµœì†Œ 10ê°œ, ìµœëŒ€ 300ê°œ
        print(f"\nğŸ“ {file_name} ({len(df)}ê±´) â†’ {sample_size}ê°œ ìƒ˜í”Œ ë¶„ì„...")

        if len(df) > sample_size:
            sample_df = df.sample(n=sample_size, random_state=42)
        else:
            sample_df = df

        for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc="ìƒ˜í”Œ ë¶„ì„", leave=False):
            result = analyze_with_gpt5(client, row['text'], row.get('rating', 5))

            if result is None:
                continue

            total_cost += result.get("cost", 0)

            aspects_data = result.get("aspects", [])
            for asp_data in aspects_data:
                aspect = asp_data.get("aspect")
                sentiment = asp_data.get("sentiment")
                keywords = asp_data.get("keywords", [])

                if aspect in ASPECTS and sentiment in ["positive", "neutral", "negative"]:
                    patterns[aspect][sentiment].extend(keywords)
                    all_samples.append({
                        'file': file_name,
                        'text': row['text'][:50],
                        'aspect': aspect,
                        'sentiment': sentiment,
                        'score': asp_data.get('score', 0),
                        'keywords': keywords
                    })

    # íŒ¨í„´ ì •ë¦¬: ë¹ˆë„ 2íšŒ ì´ìƒ í‚¤ì›Œë“œë§Œ
    refined_patterns = {}
    for aspect, sent_dict in patterns.items():
        refined_patterns[aspect] = {}
        for sentiment, kws in sent_dict.items():
            kw_counts = defaultdict(int)
            for kw in kws:
                if kw and len(kw) >= 2:
                    kw_counts[kw.lower()] += 1
            refined_patterns[aspect][sentiment] = [kw for kw, cnt in kw_counts.items() if cnt >= 2]

    return refined_patterns, all_samples, total_cost


def apply_patterns_to_all(all_dfs: dict, patterns: dict):
    """íŒ¨í„´ì„ ì „ì²´ ë°ì´í„°ì— ì ìš©"""
    results = []

    for file_name, df in all_dfs.items():
        print(f"\nğŸ“ {file_name} íŒ¨í„´ ì ìš© ì¤‘...")

        for idx, row in tqdm(df.iterrows(), total=len(df), desc="ì ìš©", leave=False):
            text = str(row['text']).lower()
            rating = row.get('rating', 5)

            detected_aspects = []

            for aspect, sent_patterns in patterns.items():
                best_match = None
                best_score = 0

                for sentiment, keywords in sent_patterns.items():
                    matched = [kw for kw in keywords if kw in text]
                    if len(matched) > best_score:
                        best_score = len(matched)
                        best_match = {
                            'aspect': aspect,
                            'sentiment': sentiment,
                            'matched_keywords': matched,
                            'score': 0.8 if sentiment == 'positive' else (-0.8 if sentiment == 'negative' else 0.0)
                        }

                if best_match and best_score >= 1:
                    detected_aspects.append(best_match)

            # ê¸°ì¡´ aspect_labelsë„ í™•ì¸
            old_labels = str(row.get('aspect_labels', ''))
            old_aspects = [l.strip() for l in old_labels.split(',') if l.strip()]

            results.append({
                'file': file_name,
                'idx': idx,
                'text': row['text'],
                'rating': rating,
                'old_sentiment': row.get('sentiment', ''),
                'old_score': row.get('sentiment_score', 0),
                'old_aspects': old_aspects,
                'new_aspects': detected_aspects
            })

    return results


def save_new_format(results: list, output_path: Path):
    """ìƒˆ í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    rows = []
    for r in results:
        if r['new_aspects']:
            for asp in r['new_aspects']:
                rows.append({
                    'text': r['text'],
                    'rating': r['rating'],
                    'aspect': asp['aspect'],
                    'sentiment': asp['sentiment'],
                    'score': asp['score'],
                    'matched_keywords': ', '.join(asp.get('matched_keywords', []))
                })
        else:
            # íŒ¨í„´ ë§¤ì¹­ ì•ˆ ëœ ê²½ìš° ê¸°ì¡´ ë°ì´í„° ìœ ì§€
            for old_asp in r['old_aspects']:
                rows.append({
                    'text': r['text'],
                    'rating': r['rating'],
                    'aspect': old_asp,
                    'sentiment': r['old_sentiment'],
                    'score': r['old_score'],
                    'matched_keywords': ''
                })

    df = pd.DataFrame(rows)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    return df


def main():
    print("="*70)
    print("ABSA íŒ¨í„´ ê¸°ë°˜ ì¬ë¶„ì„")
    print("1ë‹¨ê³„: GPT-5 ìƒ˜í”Œ ë¶„ì„ â†’ íŒ¨í„´ ì¶”ì¶œ")
    print("2ë‹¨ê³„: íŒ¨í„´ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ì ìš©")
    print("="*70)

    if "OPENAI_API_KEY" not in os.environ:
        print("Error: OPENAI_API_KEY í•„ìš”")
        return

    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

    # ëª¨ë“  íŒŒì¼ ë¡œë“œ
    all_dfs = {}
    csv_files = sorted(aspect_dir.glob("*.csv"))
    csv_files = [f for f in csv_files if "correction" not in f.name and "all_" not in f.name]

    total_reviews = 0
    for filepath in csv_files:
        df = pd.read_csv(filepath)
        all_dfs[filepath.stem] = df
        total_reviews += len(df)

    print(f"\nì´ {len(csv_files)}ê°œ íŒŒì¼, {total_reviews}ê±´ ë¦¬ë·°")

    # 1ë‹¨ê³„: íŒ¨í„´ ì¶”ì¶œ
    print("\n" + "="*70)
    print("1ë‹¨ê³„: ìƒ˜í”Œ GPT-5 ë¶„ì„")
    print("="*70)

    patterns, samples, cost = extract_patterns_from_samples(client, all_dfs, sample_ratio=0.1, max_samples=300)

    print(f"\nì¶”ì¶œëœ íŒ¨í„´:")
    for aspect, sent_dict in patterns.items():
        print(f"\n  {aspect}:")
        for sentiment, keywords in sent_dict.items():
            if keywords:
                print(f"    {sentiment}: {keywords[:5]}...")

    print(f"\n1ë‹¨ê³„ ë¹„ìš©: ${cost:.4f}")

    # íŒ¨í„´ ì €ì¥
    pattern_path = output_dir / "absa_patterns.json"
    with open(pattern_path, 'w', encoding='utf-8') as f:
        json.dump({
            'patterns': patterns,
            'samples': samples[:200],
            'cost': cost
        }, f, ensure_ascii=False, indent=2)
    print(f"íŒ¨í„´ ì €ì¥: {pattern_path}")

    # 2ë‹¨ê³„: íŒ¨í„´ ì ìš©
    print("\n" + "="*70)
    print("2ë‹¨ê³„: ì „ì²´ ë°ì´í„°ì— íŒ¨í„´ ì ìš©")
    print("="*70)

    results = apply_patterns_to_all(all_dfs, patterns)

    # ê²°ê³¼ ì €ì¥
    output_path = output_dir / "absa_results.csv"
    df_result = save_new_format(results, output_path)

    print(f"\nê²°ê³¼ ì €ì¥: {output_path}")
    print(f"ì´ {len(df_result)}ê±´ (aspect-sentiment ìŒ)")

    # í†µê³„
    print("\n[Aspectë³„ Sentiment ë¶„í¬]")
    if 'aspect' in df_result.columns and 'sentiment' in df_result.columns:
        pivot = df_result.groupby(['aspect', 'sentiment']).size().unstack(fill_value=0)
        print(pivot)

    # ì˜ì‹¬ ì¼€ì´ìŠ¤ ì¶”ì¶œ (2ë‹¨ê³„ìš©)
    suspicious = []
    for r in results:
        rating = r['rating']
        for asp in r['new_aspects']:
            sentiment = asp['sentiment']
            # ë³„ì  ë¶ˆì¼ì¹˜
            if rating >= 4 and sentiment == 'negative':
                suspicious.append({**r, 'reason': f'ë³„ì  {rating} but negative'})
            elif rating <= 2 and sentiment == 'positive':
                suspicious.append({**r, 'reason': f'ë³„ì  {rating} but positive'})

    suspicious_path = output_dir / "suspicious_cases.json"
    with open(suspicious_path, 'w', encoding='utf-8') as f:
        json.dump(suspicious[:500], f, ensure_ascii=False, indent=2, default=str)

    print(f"\nì˜ì‹¬ ì¼€ì´ìŠ¤: {len(suspicious)}ê±´ â†’ {suspicious_path}")
    print(f"(2ë‹¨ê³„ì—ì„œ GPT-4oë¡œ ê²€ì¦ ì˜ˆì •)")

    print(f"\nì´ ë¹„ìš©: ${cost:.4f}")


if __name__ == "__main__":
    main()
